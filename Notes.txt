Docker image is the enviroment which has been created. Image is a snapshot. Everything that app requires to run.
docker pull nginx -> this particular command is used to get nginx image to local. This brings the latest image.
docker images-> used to get all the downloaded images.
These images which are getting downloaded are hashed so that we don't have to download the image always.

docker run -d nginx-> This runs the nginx image in the detached mode.
docker container ls can be used to view the running image.

docker run -d -p 8080:80 nginx-> This will run the nginx container in port 80 and will expose it to 8080 in os.
We can even expose multiple ports from the current os to the container.
Here we are passing the id of all the container as per the parameters here we are simply using the output of one command to some other command.
docker rm $(docker ps -ap)

using the --name flag we can give the name to the container that is provided to us.
we can use the format command in order to view the information about the containers in a formatted way.

In case of the docker containers the data which is stored is not persistent. In order to make this persitent we require the persistent volume.
Here the volume is used in order to share the data among the container and the host.

docker run --name website -d -v $(pwd):/usr/share/nginx/html -p 8080:80 nginx
This command can be used for creating a particular container as per our requirement. In that particular container the volume would be mounted as per our requirement.
While creating the volume in case of the docker containers we can create it even by using the read only flag. This means that we cannot create files in the container itself.

We can build our own website and can add that to the persistent volume.
docker run --name website --volumes-from website -d -p 8081:80 nginx
Let us say that we have an already running container in which our current website is hosted. The volume which has been specified in that particular case can even be used over here.

We can create our own docker image as per our requirement. Here we can simply specify the image.
Images should contain all the things that we need to learn. 

Here we have created a docker file with certain commands and then we will be creating our own image for the website.
We have created the image for the github finder website and have deployed it in nginx in local environment.

docker build --tag githubfinder:latest .
This will search for the Dockerfile in the current repostory.

docker build --tag user-service-api:latest .
pulls the latest image of node.js and then simply runs the docker file.

WE can even ignore the step of including the node_modules into our docker file.

Here few of the files which we are including can be ignored as per our requirement.
Now in this particular case the files which are needed during the run time can be downloaded from the internet.

The layers while creating the docker images are cached therefore the next time step is very quick.

We can do something better rather then adding all the files into the 

All the images that we are currently using are very large in size therefore for the smaller size images we will be using the alpine images of theese.

Tags and the versioning in case of the docker images is easier. If we want to upgrade the node version then while creating the docker file we will be specifying the version of the node that is required to us.
We can decide whether we want to switch to node 8 or node 12
8-alpine 12-alpine. These information needs to be mentioned while pulling the image as per our requirement.
When we try using the latest tag with all our images then that overrides the information that we currently have. Therefore in order to avoid that we need to do the versioning of all the information that is present.

latest will point to the latest version of the website if we want to create the versioning form the latest tag we need to use these commands.
docker tag website:latest website:1
// This will simply create a new image with just different tag.

Here we are simply using the alpine image of the code which is present.
Here we will be having all the informations which is required.

docker tag githubfinder:latest githubfinder:v1.
This command will link up the latest  to the github finder tag.

Docker registeries. It is basically a server side application where all the images that we have created are stored.
These docker images are required for the CI/CD pipelines.

The docker registeries are mainly of two types private and public.
From the docker hub we can create our own private repoistory and push our image there as per our need.

docker push repname:website


docker inspect container_name is for finding the information pertaining to a particular container.

docker logs -f container_id or container_name fetches us the logs of the container itself.
It is very tedious task in order to maintain the containers which are present at the different locations.



Kubernetes was founded by the google it is an orchestration tool which is required to manage the docker pods that has been created.
The kuberenetes basically provides us the high availability as per our requirement.

In case of the kubernetes basic architecture there is a master node and there are many worker nodes in which the kubelet processes are running.
Each of the worker node which is present has the applications present at them.
The etcd in the kubernetes master node has the components which are present. Using the etcd snapshot we can restore most of the information about the containers which are running.
In case of the production enviroment there are multiple master nodes present.

We say that there are pods present inside the vm. Each of these pods have their own ip addresses and there are containers which are running inside these pods.
We have the service which are present in the kubernetes environment in case of the service we can assign the permanent ip addresses to all the values which are present.
External service is the service which is being exposed to the internet.
Service has the direct url and the port opening. There is one more component which interacts with the service it is ingress. The work of ingress is that it converts the domain name and sends it to the service.
For the database w have the internal service.
There are few informations which are very small that we don't require a deployment for those. These information can be stored insdiet he config map of the kubernetes container. Databse username and the password may also change as per the requirement.

the credentials of the db user are confidential therefore they will be stored in the form of secret.

For all the external configuration we use the external pods and the config maps.
The data which will be required for all the time are stored in the voluments. These volumes has the pods.

The storage where the data will be stored can be in the local storage of the node or in the remote host.
Storage can be seen as the external information to the kubernetes cluster. There is no persistence is 
For the databases which are present the persistence is always the case. In case of the kubernetes cluster we are simply replicating everything that is with us. Pod is simply the layer of abstraction over the container while the deployment is the layer of abstraction above the pod.

If the database pod dies we cannot simply replicated it. Therefore we simply can't create the replicas for it in particular.
Database have the states which is the data which is present in this case.

StatefulSet is used for most of the database applciation as the state of the data needs to be maintained in all the database pods which are being created.

Summary of the kubernetes.
Basic teminologies for the k8s environment will be.
Volumes pod service ingress Deployment StatefulSet ConfigMap and Secrets most of them are necessary for the creation of the pods.


Minikube is used in the local machine. If there are not much resources then for the simple implementation the minikube can be used.
Minikube is the software in which the master processes and the worker processes work on the same machine.

Minikube simply creates a virtual box in our local environment as per our requirement.
kubectl is used for configuring the pods which are present inside the machine itself.

There is always a master process in case of the kuberenetes environment which is responsible for communicating with the other environment which are present.

We can configure brew in our local environment and then use it to install brew in the local environemnt.
brew install hyperkit -> IT is basicaly used as the hypervisor.
brew install minikube -> Installs both minikube and the kubectl.

In order to start the minikube cluster with the single node setup we will be using this particular command.

minikube start --vm-driver=hyperkit
We will have to configure the virtualization in our machine in order to do it.
kubectl get pods gets us the status of the minikube pod which is currently running it will be the master node.

minikube is simply used to start and stop the cluster all the other things are done by the kubectl.
check whether the minukube is running or not minikube status is used.

Create and delete pods in the kubectl environment.

In the kubernetes we have pods as the smallest unit but we don't directly work with the pods. Therefore we create the deployments as per our requirement.

kubectl create deployment NAME --image=image [--dry-run] [options]
Here for creating a particular deployment we need to specify the image that will be required by us.

kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-6799fc88d8-bffcw   0/1     ContainerCreating   0          4s
kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl create deployment mongo-depl --image=mongo
deployment.apps/mongo-depl created
kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl get pods
NAME                          READY   STATUS              RESTARTS   AGE
mongo-depl-5fd6b7d4b4-w52lh   0/1     ContainerCreating   0          5s
nginx-6799fc88d8-bffcw        0/1     ContainerCreating   0          22s

Here we can see that the containers has been created as per our requirement.

Here kubectl logs <POD NAME>-> This particular command can be used for fetching the information regarding the pods as per our requirement.
kubectl describe <POD NAME> this command can be used for getting the information about the status of the pods which are currently being runnning.

In order to get more information what happened with the deployment we can get inside the pods which are being created by the kubectl.
kubectl exec -it <POD NAME> -- /bin/bash

kubectl delete deployment <Deployment Name> this particular command is used for deleting the particular deployment as per our requirement.

There are many tasks that we want to do along with the pod creation for this purpose we require to create the deployment file as per our need.

kubectl apply -f config-file.yaml
Here the changes that are being made are being made to the current deployment. Let us say that we have increase the replica count to 3 then there will be total of three replicas this will not 
be creating 3 new replicas. Since currently there are two it will create one more replica for that deploment.

PROJECT WORK HOSTING THE MONGODB AND EXPRESS APPLICAION IN THE LOCAL ENVIRONMENT
initially we will be creating the deployment for the mongodb setup and in that deployment we will create the internal service as we want database to be accessible by the services which are inside.
CONFIG MAP AND THE SECRETS WILL BE CREATED. 
for the application itself we will be creating one external service so that it is accessible in the browser as well.


The entire flow for the applcation will go like this
The application when opened in UI will use mongoexpress external service to interact with the internal service mongodb.

The username and the password which is being used is the base64 encoded as per our requirement.
We can connect a particular service with the pods using the selector tool here we specify the label that was used during the deployment.
targetPort: is the port of the container or the Pod while the port is the port of which needs to be exposed.

The namespaces are simply the virtual cluster inside a big cluster. These are entirely different scenario.
Here The name of the selector in case of the mongodb will be the same as the label of the deployment we have created.

kubectl get service will be used for fetching the information about the services that we have created on our own.
Here we can describe the service and can know with which particular deployment it is associated.

kubectl get all-> Can be used for getting all the possible information which is possible.

inside the config map we have stored the database url which is required.
here we can edit an already running pod using kubectl edit pods <pod name>

We don't directly deal with the pods while we deal with containers in which the pods are contained.
Here the replication controller is the controller that is responsible for the creation of the replica set.

Replicas and the replica set are very similar in task.
ReplicaSet is mostly used for creating the replicas.

The replicaset provides the selector fields which during the pod creation can match with a preexistsing label.

Necessity of the labels and selectors.
ReplicaSet is the application which monitors the existence of the pods.
If pods are less than the amount which was expected then replicaset itself does the deployment.
For the creation of the pod if one goes down then we will simply have to 

If we later want to scale the number of the replica set which are present we can use the following command.
kubectl replace -f replicaset-definition.yaml
kubectl scale --replicas=6 -f replicaset-definition.yaml

While working on with the kubernetes we majorly work with the deployment which takes care of the replica set and the other stuff.
Rolling updates are used.
In case of every new deployments there are rollout and the versioning are performed.

kubectl set image deployment myapp-deployment <imagename> command can be used in order to change the running image of the deployment which we are using.
kubectl rollout undo deployment/myapp-deployment

Kubernetes:
In case of the kuberenetes environment each of the pods get their own IP address.
Kubernetes whave the ips  which is required for linking the kubernetes nodes. Pod when created gets an ip address while the creation of the pod the kubernetes container will get a network ip address here all the private ips are inside this particular network.
Kubernetes suggests us to setup the networking for the kubernetes cluster that we currently have.

The kubernetes pods that we have created are in a different network therefore we need services in order to acccess those nodes as per our requirement.
1) NodePort:- we will map a port on the node to the port which is in the pod.
The targetPort is the port where the service redirects our requests to.

For balancing the load among all the pods which are present it uses the random algorithm.
Service only needs to be configured once if the service has been configured well then there should not be any extra confiuration for the service.
In case of the cluster ip configuration we will be simply having a cluster ip mentioned as per our requirement.

Example voting app in kubernetes.
Goals for developing the application are as follows.
1) Deploy containers
2) Enable Connectivity within the containers.

Steps that needs to be followed for the application creation are as follos:
1) Deploy PODS
2) create services (ClusetIP)
. redist and the voting app.
service by postgres is created.

We will be creating the services for the voting app and the result-app as they are to be accessed by the end users.