Docker image is the enviroment which has been created. Image is a snapshot. Everything that app requires to run.
docker pull nginx -> this particular command is used to get nginx image to local. This brings the latest image.
docker images-> used to get all the downloaded images.
These images which are getting downloaded are hashed so that we don't have to download the image always.

docker run -d nginx-> This runs the nginx image in the detached mode.
docker container ls can be used to view the running image.

docker run -d -p 8080:80 nginx-> This will run the nginx container in port 80 and will expose it to 8080 in os.
We can even expose multiple ports from the current os to the container.
Here we are passing the id of all the container as per the parameters here we are simply using the output of one command to some other command.
docker rm $(docker ps -ap)

using the --name flag we can give the name to the container that is provided to us.
we can use the format command in order to view the information about the containers in a formatted way.

In case of the docker containers the data which is stored is not persistent. In order to make this persitent we require the persistent volume.
Here the volume is used in order to share the data among the container and the host.

docker run --name website -d -v $(pwd):/usr/share/nginx/html -p 8080:80 nginx
This command can be used for creating a particular container as per our requirement. In that particular container the volume would be mounted as per our requirement.
While creating the volume in case of the docker containers we can create it even by using the read only flag. This means that we cannot create files in the container itself.

We can build our own website and can add that to the persistent volume.
docker run --name website --volumes-from website -d -p 8081:80 nginx
Let us say that we have an already running container in which our current website is hosted. The volume which has been specified in that particular case can even be used over here.

We can create our own docker image as per our requirement. Here we can simply specify the image.
Images should contain all the things that we need to learn. 

Here we have created a docker file with certain commands and then we will be creating our own image for the website.
We have created the image for the github finder website and have deployed it in nginx in local environment.

docker build --tag githubfinder:latest .
This will search for the Dockerfile in the current repostory.

docker build --tag user-service-api:latest .
pulls the latest image of node.js and then simply runs the docker file.

WE can even ignore the step of including the node_modules into our docker file.

Here few of the files which we are including can be ignored as per our requirement.
Now in this particular case the files which are needed during the run time can be downloaded from the internet.

The layers while creating the docker images are cached therefore the next time step is very quick.

We can do something better rather then adding all the files into the 

All the images that we are currently using are very large in size therefore for the smaller size images we will be using the alpine images of theese.

Tags and the versioning in case of the docker images is easier. If we want to upgrade the node version then while creating the docker file we will be specifying the version of the node that is required to us.
We can decide whether we want to switch to node 8 or node 12
8-alpine 12-alpine. These information needs to be mentioned while pulling the image as per our requirement.
When we try using the latest tag with all our images then that overrides the information that we currently have. Therefore in order to avoid that we need to do the versioning of all the information that is present.

latest will point to the latest version of the website if we want to create the versioning form the latest tag we need to use these commands.
docker tag website:latest website:1
// This will simply create a new image with just different tag.

Here we are simply using the alpine image of the code which is present.
Here we will be having all the informations which is required.

docker tag githubfinder:latest githubfinder:v1.
This command will link up the latest  to the github finder tag.

Docker registeries. It is basically a server side application where all the images that we have created are stored.
These docker images are required for the CI/CD pipelines.

The docker registeries are mainly of two types private and public.
From the docker hub we can create our own private repoistory and push our image there as per our need.

docker push repname:website


docker inspect container_name is for finding the information pertaining to a particular container.

docker logs -f container_id or container_name fetches us the logs of the container itself.
It is very tedious task in order to maintain the containers which are present at the different locations.



Kubernetes was founded by the google it is an orchestration tool which is required to manage the docker pods that has been created.
The kuberenetes basically provides us the high availability as per our requirement.

In case of the kubernetes basic architecture there is a master node and there are many worker nodes in which the kubelet processes are running.
Each of the worker node which is present has the applications present at them.
The etcd in the kubernetes master node has the components which are present. Using the etcd snapshot we can restore most of the information about the containers which are running.
In case of the production enviroment there are multiple master nodes present.

We say that there are pods present inside the vm. Each of these pods have their own ip addresses and there are containers which are running inside these pods.
We have the service which are present in the kubernetes environment in case of the service we can assign the permanent ip addresses to all the values which are present.
External service is the service which is being exposed to the internet.
Service has the direct url and the port opening. There is one more component which interacts with the service it is ingress. The work of ingress is that it converts the domain name and sends it to the service.
For the database w have the internal service.
There are few informations which are very small that we don't require a deployment for those. These information can be stored insdiet he config map of the kubernetes container. Databse username and the password may also change as per the requirement.

the credentials of the db user are confidential therefore they will be stored in the form of secret.

For all the external configuration we use the external pods and the config maps.
The data which will be required for all the time are stored in the voluments. These volumes has the pods.

The storage where the data will be stored can be in the local storage of the node or in the remote host.
Storage can be seen as the external information to the kubernetes cluster. There is no persistence is 
For the databases which are present the persistence is always the case. In case of the kubernetes cluster we are simply replicating everything that is with us. Pod is simply the layer of abstraction over the container while the deployment is the layer of abstraction above the pod.

If the database pod dies we cannot simply replicated it. Therefore we simply can't create the replicas for it in particular.
Database have the states which is the data which is present in this case.

StatefulSet is used for most of the database applciation as the state of the data needs to be maintained in all the database pods which are being created.

Summary of the kubernetes.
Basic teminologies for the k8s environment will be.
Volumes pod service ingress Deployment StatefulSet ConfigMap and Secrets most of them are necessary for the creation of the pods.


Minikube is used in the local machine. If there are not much resources then for the simple implementation the minikube can be used.
Minikube is the software in which the master processes and the worker processes work on the same machine.

Minikube simply creates a virtual box in our local environment as per our requirement.
kubectl is used for configuring the pods which are present inside the machine itself.

There is always a master process in case of the kuberenetes environment which is responsible for communicating with the other environment which are present.

We can configure brew in our local environment and then use it to install brew in the local environemnt.
brew install hyperkit -> IT is basicaly used as the hypervisor.
brew install minikube -> Installs both minikube and the kubectl.

In order to start the minikube cluster with the single node setup we will be using this particular command.

minikube start --vm-driver=hyperkit
We will have to configure the virtualization in our machine in order to do it.
kubectl get pods gets us the status of the minikube pod which is currently running it will be the master node.

minikube is simply used to start and stop the cluster all the other things are done by the kubectl.
check whether the minukube is running or not minikube status is used.

Create and delete pods in the kubectl environment.

In the kubernetes we have pods as the smallest unit but we don't directly work with the pods. Therefore we create the deployments as per our requirement.

kubectl create deployment NAME --image=image [--dry-run] [options]
Here for creating a particular deployment we need to specify the image that will be required by us.

kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-6799fc88d8-bffcw   0/1     ContainerCreating   0          4s
kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl create deployment mongo-depl --image=mongo
deployment.apps/mongo-depl created
kunal@kunal-Inspiron-5559:~/hyperkit$ kubectl get pods
NAME                          READY   STATUS              RESTARTS   AGE
mongo-depl-5fd6b7d4b4-w52lh   0/1     ContainerCreating   0          5s
nginx-6799fc88d8-bffcw        0/1     ContainerCreating   0          22s

Here we can see that the containers has been created as per our requirement.

Here kubectl logs <POD NAME>-> This particular command can be used for fetching the information regarding the pods as per our requirement.
kubectl describe <POD NAME> this command can be used for getting the information about the status of the pods which are currently being runnning.

In order to get more information what happened with the deployment we can get inside the pods which are being created by the kubectl.
kubectl exec -it <POD NAME> -- /bin/bash

kubectl delete deployment <Deployment Name> this particular command is used for deleting the particular deployment as per our requirement.

There are many tasks that we want to do along with the pod creation for this purpose we require to create the deployment file as per our need.

kubectl apply -f config-file.yaml
Here the changes that are being made are being made to the current deployment. Let us say that we have increase the replica count to 3 then there will be total of three replicas this will not 
be creating 3 new replicas. Since currently there are two it will create one more replica for that deploment.

PROJECT WORK HOSTING THE MONGODB AND EXPRESS APPLICAION IN THE LOCAL ENVIRONMENT
initially we will be creating the deployment for the mongodb setup and in that deployment we will create the internal service as we want database to be accessible by the services which are inside.
CONFIG MAP AND THE SECRETS WILL BE CREATED. 
for the application itself we will be creating one external service so that it is accessible in the browser as well.


The entire flow for the applcation will go like this
The application when opened in UI will use mongoexpress external service to interact with the internal service mongodb.

The username and the password which is being used is the base64 encoded as per our requirement.
We can connect a particular service with the pods using the selector tool here we specify the label that was used during the deployment.
targetPort: is the port of the container or the Pod while the port is the port of which needs to be exposed.

The namespaces are simply the virtual cluster inside a big cluster. These are entirely different scenario.
Here The name of the selector in case of the mongodb will be the same as the label of the deployment we have created.

kubectl get service will be used for fetching the information about the services that we have created on our own.
Here we can describe the service and can know with which particular deployment it is associated.

kubectl get all-> Can be used for getting all the possible information which is possible.

inside the config map we have stored the database url which is required.
here we can edit an already running pod using kubectl edit pods <pod name>

We don't directly deal with the pods while we deal with containers in which the pods are contained.
Here the replication controller is the controller that is responsible for the creation of the replica set.

Replicas and the replica set are very similar in task.
ReplicaSet is mostly used for creating the replicas.

The replicaset provides the selector fields which during the pod creation can match with a preexistsing label.

Necessity of the labels and selectors.
ReplicaSet is the application which monitors the existence of the pods.
If pods are less than the amount which was expected then replicaset itself does the deployment.
For the creation of the pod if one goes down then we will simply have to 

If we later want to scale the number of the replica set which are present we can use the following command.
kubectl replace -f replicaset-definition.yaml
kubectl scale --replicas=6 -f replicaset-definition.yaml

While working on with the kubernetes we majorly work with the deployment which takes care of the replica set and the other stuff.
Rolling updates are used.
In case of every new deployments there are rollout and the versioning are performed.

kubectl set image deployment myapp-deployment <imagename> command can be used in order to change the running image of the deployment which we are using.
kubectl rollout undo deployment/myapp-deployment

Kubernetes:
In case of the kuberenetes environment each of the pods get their own IP address.
Kubernetes whave the ips  which is required for linking the kubernetes nodes. Pod when created gets an ip address while the creation of the pod the kubernetes container will get a network ip address here all the private ips are inside this particular network.
Kubernetes suggests us to setup the networking for the kubernetes cluster that we currently have.

The kubernetes pods that we have created are in a different network therefore we need services in order to acccess those nodes as per our requirement.
1) NodePort:- we will map a port on the node to the port which is in the pod.
The targetPort is the port where the service redirects our requests to.

For balancing the load among all the pods which are present it uses the random algorithm.
Service only needs to be configured once if the service has been configured well then there should not be any extra confiuration for the service.
In case of the cluster ip configuration we will be simply having a cluster ip mentioned as per our requirement.

Example voting app in kubernetes.
Goals for developing the application are as follows.
1) Deploy containers
2) Enable Connectivity within the containers.

Steps that needs to be followed for the application creation are as follos:
1) Deploy PODS
2) create services (ClusetIP)
. redist and the voting app.
service by postgres is created.

We will be creating the services for the voting app and the result-app as they are to be accessed by the end users.

CKAD course for the kubernetes certification.
Controller are the brain of the kubernetes which means that all the information are maintained using the kubernetes containers only.

Replication controller makes sure that the specified number of pods are running at all the times as mentioned.
Replication container is also required to share the load among all the pods which are present.

All the operations whichh we have performed lately are performed using the namespace only the operations which were currently being done were inside the default namespace.
networks are inside the kubesystem
kube-public are all the public resources.

We can define the quota limit for the namespaces as per our requirement.
By default the image that we have created does not have the bash associated with it as per our requirement.

docker run ubuntu [COMMAND]
this basically runs the docker container with the ubuntu image in it and inside that particular image it runs the sleep command as per our requirement.
CMD command param1

Entrypoint can be specified in the docker file in which me can specify the params inside the console itself.
FROM Ubuntu 
ENTRYPOINT ["sleep"]
CMD ["5"]
This value 5 will be acting as the parameter as per our own requirement.

docker run ubuntu-sleeper 5
Here if we want to create a secret then we will simply using this particular command.
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
// This will create the generic secrets.

docker run --user=1000 ubuntu sleep 3600
this will run the sleep command with user as 1000.

docker run --cap-add 
docker run --privileged

Settings that we will do on a container will override the settings that has been done in the pod.
we can prevent the automounting of the service account as per our requirement.
autoMountServiceAccount: false -> THis needs to be set to false for the pods.

In case of te pods which are being created by the deployments we can specify the max CPU it can use and the max memory.
If the memory exceeds then the pod will be restarted.
OOMKilled status is being used when the pod runs out of the memory that is required by it.

Taints and toleration:
THis concept is used if we want to put certain pods at certain nodes as per our requirement.
we apply the taints to the nodes so that no pods are gone to that node then we apply toleration to containers so that they can be scheduled on that node.
Initially if some taint is applied to a node then none of the pods are tolerant to that taint. we need to specify the toleration in the pods for the specific taints as per our requirement.

kubectl taint nodes node-name key=value:taint-effect
there are three types of taint-effect
NoSchedule:- no pods with specific toleration will be scheduled.
PreferNoSchedule:- pod can be scheduled
NoExecute:- previous nodes which do not tolerate the taint are evicted.

kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-
command is used to remove the taint.

Readiness and the liveness probe is used a lot in the kubernetes
using the kubectl get pods command we can get all the pods which are currently running. 
Liveness probe is used to make sure that the application is inside the container is running without any errors.

In order to check the logs of the pods which have been recently created we use the command as 
kubectl logs -f <pod name> this command is similar to the command that we use in case of docker.

If there are multiple containers present inside the pod then we will have to specify the exact name of the container for which we need to view the logs
kubectl logs -f event-simulator-pod event-simulator

Metric server is  used to fetch data from the different kubernetes nodes and then pods aggregate them and stores then into the memory.
kubelet is used for fetching the information.

The strategy that is used a lot in case of deployments in kubernetes environment is the rolling update strategy in this particular strategy one new pod is brought up and then one old pod is removed.

kubectl set image deploment/myapp-deployment nginx-container=nginx:1.12
This particular command with upgrade the version of nginx as per our requirement.

kubectl rollout status deployment/nginx

kubectl rollout undo deployment/myapp-deployment

basically the main use of the jobs that we have created is that they perform a certain task and after completing that particular task a few number of times the jobs are deleted.
But pods are maintained.

There are mainly three types of services which are used. The first type of service is 
1) NodePort
2) ClusterIP
3) ingress

In case of the node port the application needs to be viewed in the ui as per our requirement therefore we will have to specify a node port 
for a service which will redirect the traffic to port where the service is running which in place will redirect the traffic to pod itself 
and hence we will be able to access the application as per our requirement,

ClusterIP scenario is used when we have many services which are running in a particular namespace and these services in particular interacts with each other as per our requirement.

In case of the ingress we can perform the ssl security as per our requriement and we can define a hostname as well as per our requirement as remembering the hostname is easier than remembering IP address.

in order to use the ingress functionality we will have to deploy the ingress controller as per our need.
kubectl get ingress --all-namespace

kubectl describe ingress ingress-wear-watch -n app-space

For creating a particular ingress controller firstly we need to deploy the pod of the ingress controller and then expose it using the service file.
Command which will be used.
kubectl create cm nginx-configuration -n ingress-space
this will create a configmap.

kubectl get networkpolicy

If we want to permanently store the data into the host then we will have to create a volume.
Persistent volume claim cannot be delete if it is currently being used by a pod.

The main need of statefulset is that the name of the pods changes after a certain time.

Similar commands can be used in case of the stateful sets as well.
Dockerfile is some set of instructions that we provide to happen one after other. this is used for creating our own image.

Secure hosts->
Password based authentication disabled
SSH key based authentication.

The first line of defence is controlling access to the api server itself.
Authorization is done by introducing rule based access controls.

There are admins and developers and then there are end users.
Securing access to the kubernetes cluster using the authentication and authorization mechanisms.

All the users accesses are managed by api-server.
curl https://kube-server:9131

the username and password can be on the static password file or they can be in a static token file.
Certificates can also be used for authentication to the kubeapi server.
user-details are being passed in user-details.csv file.

by changing the kubeapiserver file.
username and password are passed into the curl request then.

curl https://my-kube-playground:6443/api/v1/pods \
--key admin.key
--cert admin.crt
--cacert ca.crt

We can create a kubeconfig file as per our requirement.

we don't have to specify the configuration.
Contexts Admin@rproduction.
mykubeadmin user to acccessm mykubeplayground user.
Kubeconfig file is used for specifying the information regarding the pods.
we can define users which can view pods and the other things as well as specified.

If the authorization is set to AlwaysAllow then all the things are always allowed.
RBAC checks the rules and then actually try to authorizing the person.

RBAC:
In case of RBAC we can define the roles.
kubectl describe pod kube-apiserver-controlplane -n kube-system | grep authorization
// this helps us in fetching the information regarding the rule based access control.

kubectl get pods --as dev-user
this particular command can be used for running it as the user as we want to.

To create a Role:- kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods
To create a RoleBinding:- kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
OR

the alpha1 version of api means that the  api has been created before and then it is merged with the kubernetes code.
The apiGroups which are in beta stage and enabled by default.
after these it moves to stable GA.
a group may have different groups.

To enable a particular API group we need to confugure it to the startup of apiserver.
then restart the apiserver services.

API DEPRICATION POLICY:
API objects must be able to round-trip between API versions in a given release without information loss,
with the exception of while REST resources that do not exist in some versions.
v1alpha1 v1beta1 v1beta2 then v1.
this is particular api is developed.

There should be a deprication policy in order to remove the older versions.
If a new alpha release is introduced then the older versions are removed one after the other.

In case of beta version this is deprecated after 3 months.
we change the preferred version.

kubectl convert *.yaml --output-version apps/v1
this will change the version
The kubectl convert needs to be installed.

cp -v /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup
// this is used for making the backup of the apiserver.


Run the command kubectl-convert to change the deprecated API version as follows :-

root@controlplane:~# kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1

# store new changes into a file 
root@controlplane:~# kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml

After changing the API version and storing into a file, use the kubectl create -f command to deploy the resource :-

root@controlplane:~# kubectl create -f ingress-new.yaml

Inspect the apiVersion as follows :-

root@controlplane:~# kubectl get ing ingress-space -oyaml | grep apiVersion

Note: Maybe you will not see the service and other resources mentioned in the ingress YAML on the controlplane node because we have to only deploy the ingress resource with the latest API version.
There are two types of the deployment strategies which are used majorly. Those two approaches are.
1) Blue Green strategy
... there is one more similar reployment created with the same number of replicas but a different label.
Then we will directly try to switch traffic to new deployment.

in case of canary approach the traffic is shifted one after the other not all at once.

Helm is basically used in order to make the creation of the  deploments easier for us as per our own requirement.
helm install wordpress -> This is run all the kubernetes space.